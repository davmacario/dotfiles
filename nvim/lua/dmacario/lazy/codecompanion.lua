return {
	"olimorris/codecompanion.nvim",
	dependencies = {
		"nvim-lua/plenary.nvim",
		"nvim-treesitter/nvim-treesitter",
	},
	config = function()
		require("codecompanion").setup({
			-- display.chat.show_settings = true
			display = {
				chat = {
					show_settings = true,
					show_header_separation = true,
					-- Change the default icons
					icons = {
						pinned_buffer = "Ôêµ ",
						watched_buffer = "üëÄ ",
					},

					-- Alter the sizing of the debug window
					debug_window = {
						---@return number|fun(): number
						width = vim.o.columns - 5,
						---@return number|fun(): number
						height = vim.o.lines - 2,
					},

					-- Options to customize the UI of the chat buffer
					window = {
						layout = "vertical", -- float|vertical|horizontal|buffer
						position = "right", -- left|right|top|bottom (nil will default depending on vim.opt.plitright|vim.opt.splitbelow)
						border = "single",
						height = 0.8,
						width = 0.30,
						relative = "editor",
						opts = {
							breakindent = true,
							cursorcolumn = false,
							cursorline = false,
							foldcolumn = "0",
							linebreak = true,
							list = false,
							numberwidth = 1,
							signcolumn = "no",
							spell = false,
							wrap = true,
						},
					},
				},
			},
			adapters = {
				llama3_1 = function()
					return require("codecompanion.adapters").extend("ollama", {
						name = "llama3.1",
						env = {
							url = "http://100.91.137.78:11435",
						},
						headers = {
							["Content-Type"] = "application/json",
						},
						parameters = {
							sync = true,
						},
						schema = {
							model = {
								default = "llama3.1",
							},
							num_ctx = {
								default = 16384,
							},
							num_predict = {
								default = -1,
							},
						},
					})
				end,
				codestral = function()
					return require("codecompanion.adapters").extend("ollama", {
						name = "codestral",
						env = {
							url = "http://100.91.137.78:11435",
						},
						headers = {
							["Content-Type"] = "application/json",
						},
						parameters = {
							sync = true,
						},
						schema = {
							model = {
								default = "codestral",
							},
							num_ctx = {
								default = 16384,
							},
							num_predict = {
								default = -1,
							},
						},
					})
				end,
			},
			strategies = {
				chat = {
					adapter = "llama3_1",
				},
				inline = {
					adapter = "codestral",
				},
			},
		})
	end,
}
